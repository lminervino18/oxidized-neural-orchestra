\newpage
# Resumen
 El entrenamiento de modelos de aprendizaje profundo, requiere de recursos computacionales elevados y largos tiempos de ejecución. Junto con la revolución de la Inteligencia Artificial, se volvió un tema candente el minimizar el tiempo de entrenamiento de estos modelos. Para lograr esto último, la estrategia más escalable es su ejecución distribuida.  
Algunos de los problemas del entrenamiento distribuido son el manejo de los grandes volúmenes de datos, la sincronización del avance de cada uno de los nodos del sistema y el asegurar la convergencia.  
En base a esta problemática, en la actualidad están surgiendo muchas modificaciones de los principales algoritmos que implementan este entrenameinto distribuido. En particular, en este proyecto se estudian los dos algoritmos más relevantes: *Parameter Server* y *All-Reduce*, y sus derivados; sobre una implementación de un sistema distribuido que sirve como framework para el análisis.
