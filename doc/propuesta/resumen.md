\newpage
# Resumen
El entrenamiento de modelos de aprendizaje profundo, requiere de grandes recursos computacionales y largos tiempos de ejecución; los algoritmos que se utilizan hacen uso intensivo de CPU y requieren de capacidades de memoria muy grandes para poder almacenar tanto los datos de entrenamiento como el modelo en sí. Junto con la revolución de la aplicación de la inteligencia artificial, se ha vuelto un tema candente el minimizar éste tiempo de entrenamiento. Para lograrlo, la estrategia más escalable es su ejecución distribuida.  

El problema es que utilizar un algoritmo de optimización para entrenar un modelo de aprendizaje profundo viene acompañado tanto de desafíos a la hora de plantear la estrategia de distribución como de peores (o directamente inutilizables) resultados de convergencia si no se realiza con cuidado.  
En particular, si se optara por la distribución por datos, es decir, particionar el *dataset* entero y hacer que cada uno de los nodos del sistema distribuido iterase, por ejemplo, el algoritmo de *backpropagation* sobre la partición que lo tocó, y luego de la finalización del cómputo del algoritmo en cada uno de estos nodos se intentara hacer un *reduce* de los resultados que se calcularon por separado; el modelo no quedaría optimizado sobre la totalidad del dataset dado que cada uno de los nodos habría incurrido en mucho *overfitting*.  
Además, la agregación de soluciones especializadas para particiones de datos no puede computarse con algún método simple como el cálculo de un promedio del resultado para cada peso del modelo, ya que el resultado de cada iteración de los algoritmos de optimización que se utilizan para el entrenamiento, depende del anterior.  

A pesar de las dificultades que existen, entre ellas las ya mencionadas, existen varias soluciones distribuidas que logran sortearlas y que proveen grandes reducciones en los tiempos de ejecución de los algoritmos de entrenamiento, a partir de la suma del poder de cómputo y de la capacidad de memoria de los nodos en los sistemas que las ejecutan. En la actualidad, dos de las soluciones más llamativas son *Parameter Server* y *All Reduce*, con más reducción en el tiempo de ejecución y mejor estabilidad en la convergencia de la optimización, respectivamente.  

En este trabajo se estudian los algoritmos mencionados y sus derivados. Para ello, se busca implementar un sistema distribuido de entrenamiento de modelos de aprendizaje profundo, que funcione como base para el análisis y como punto de partida para la realización de modificaciones de los algoritmos con el objetivo de mejorar el panorama actual.
