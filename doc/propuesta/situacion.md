\newpage
# Estado del arte
El entrenamiento distribuido surge como una extensión de las técnicas de paralelización en *HPC* (High Performance Computing). Con los primeros enfoques de paralelismo de datos en frameworks como **TensorFlow** [@2021zndo4758419D] y **PyTorch** [@paszke2019pytorchimperativestylehighperformance], y sus implementaciones más recientes optimizadas para arquitecturas heterogéneas, la evolución de estos métodos refleja la tensión constante entre escalabilidad y coherencia de los parámetros del modelo.

Como ya se mencionó, los principales desafíos del desarrollo de algoritmos distribuidos de entrenamiento de modelos de aprendizaje profundo, son el encontrar una estrategia que, satisfactoriamente, disminuya el tiempo del entrenamiento o que provea una solución a la incapacidad de un flujo existente de almacenar el dataset o incluso el modelo en sí. Ésto sería mucho más sencillo si las iteraciones no fueran dependientes; podríamos simplemente utilizar una estrategia análoga a un *fork-join* y combinar la solución.  
Pero en un *gradient-descent*, la dirección de un descenso en la superficie de costo depende del punto en el que se encuentra el proceso en un momento dado, es decir, depende de la configuración de la matriz de pesos del modelo en esa iteración.

En un esquema de paralelización del entrenamiento por datos, existe el riesgo de incurrir en *overfitting* sobre las particiones si los pesos no se sincronizan con la frecuencia suficiente. Por otro lado, si la sincronización se realiza con demasiada frecuencia, el tiempo de comunicación entre los nodos puede volverse dominante. Este costo no es en absoluto despreciable: si la comunicación representa una fracción significativa del tiempo total de entrenamiento, la distribución del cómputo pierde sentido, ya que la ejecución paralela termina siendo más lenta que el entrenamiento secuencial.  

Este equilibrio entre convergencia y eficiencia de comunicación ha motivado una amplia línea de investigación. En la actualidad, existen dos enfoques fundamentales que sirven como base para el desarrollo de nuevas técnicas: los ya mencionados **Parameter Server** [@10.5555/2685048.2685095] y **All-Reduce** [@10.1145/3146347.3146350].  
En el enfoque de Parameter Server, el modelo se divide en un conjunto de parámetros centralizados (no necesariamente en un único nodo), a los cuales los nodos trabajadores envían gradientes y desde los cuales reciben actualizaciones, que surgen de la agregación de estas contribuciones. Esto permite entrenamiento asincrónico y buena escalabilidad, resultando en una reducción significativa del tiempo de entrenamiento, aunque a costa de la coherencia entre copias del modelo.  
En contraste, en All-Reduce todos los nodos intercambian gradientes de manera directa (utilizando, por ejemplo, implementaciones de anillo), garantizando sincronización exacta entre réplicas pero penalizando el rendimiento cuando el número de nodos crece.  
Uno de los claros casos de combinación de estos algoritmos es **Strategy-Switch** [@article], que inicia iterando sobre All-Reduce y, guiado por una regla empírica, sigue con Parameter Server asincrónico una vez que el modelo en entrenamiento se estabiliza; logrando así mantener la precisión del entrenamiento de *All-Reduce* y la reducción significativa del tiempo total de entrenamiento de *Parameter Server*.  

<!-- strategy-switch abstract: "...This method initiates training under the All-Reduce system and, guided by an empirical rule, transitions to asynchronous Parameter Server training once the model stabilizes. Our experimental analysis demonstrates that we can achieve comparable accuracy to All-Reduce training but with significantly accelerated training." -->

En este trabajo se estudian en profundidad ambos algoritmos base y se investigan los métodos que de ellos derivan. Y se busca a partir de dicho análisis proponer mejoras mediante la combinación de estrategias o la optimización de sus componentes de comunicación y sincronización. Además se busca desarrollar un sistema que funcione como base para el análisis comparativo del desempeño de las distintas estrategias de distribución, en términos de reducción de tiempos, convergencia y escalabilidad; y para la aplicación de las posibles mejoras que se encuentren.
